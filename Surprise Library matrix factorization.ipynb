{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load built in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import Dataset\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed),\n",
    "# data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userID  itemID  rating\n",
      "0       9       1       3\n",
      "1      32       1       2\n",
      "2       2       1       4\n",
      "3      45       2       3\n",
      "4       9       2       1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#each line needs to respect the form of -> user ; item ; rating ; [timestamp]\n",
    "\n",
    "ratings_dict = {'itemID': [1, 1, 1, 2, 2],\n",
    "                'userID': [9, 32, 2, 45, 9],\n",
    "                'rating': [3, 2, 4, 3, 1]}\n",
    "\n",
    "columns=['userID', 'itemID', 'rating']\n",
    "\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "\n",
    "df=df[columns]\n",
    "\n",
    "print(df,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data into Surprise package format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the dataset, Lowest rating is 1 and Highest rating is 4\n",
      "\n",
      "Setting rating scale accordingly\n"
     ]
    }
   ],
   "source": [
    "from surprise import Reader\n",
    "\n",
    "lower_rating=df['rating'].min()\n",
    "higher_rating=df['rating'].max()\n",
    "\n",
    "print(\"In the dataset, Lowest rating is {1} and Highest rating is {0}\".format(higher_rating,lower_rating))\n",
    "\n",
    "print(\"\\nSetting rating scale accordingly\")\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is required.\n",
    "reader = Reader(rating_scale=(lower_rating, higher_rating))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all available models [here](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html) inlcuding [ALS and SGD baselines](https://surprise.readthedocs.io/en/stable/prediction_algorithms.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f4efc3b8fd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import SVD,SVDpp\n",
    "from surprise.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "algo = SVD()\n",
    "# or \n",
    "# algo = SVDpp()\n",
    "\n",
    "algo.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a specific user_id and item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 45         item: 1          r_ui = None   est = 2.86   {'was_impossible': False}\n",
      "Estimated rating: 2.864370356352802\n"
     ]
    }
   ],
   "source": [
    "pred=algo.predict(uid=45,iid=1)\n",
    "print(pred)\n",
    "score=pred.est\n",
    "print(\"Estimated rating:\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a specific user_id and top 'n' unrated item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_id 46 hasn't rated Item_ids [1 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>2.698707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>2.514947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID    rating\n",
       "0      46       1  2.698707\n",
       "1      46       2  2.514947"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of all item IDs\n",
    "iids=df['itemID'].unique()\n",
    "\n",
    "# Get a list of Items that a perticular user has rated\n",
    "uid=46\n",
    "iids_rated=df.loc[df['userID']==uid,'itemID']\n",
    "\n",
    "# Get a list of Items that a perticular user has not rated\n",
    "iids_to_pred=np.setdiff1d(iids,iids_rated)\n",
    "\n",
    "print(\"User_id\",uid,\"hasn't rated Item_ids\",iids_to_pred)\n",
    "\n",
    "# Set sample data for model to predict\n",
    "random_rating=4 # can be any value just to complete dataframe\n",
    "test_set=[[uid,iid,random_rating] for iid in iids_to_pred]\n",
    "\n",
    "# Get model predictions as a list of prediction objects\n",
    "predictions=algo.test(test_set)\n",
    "\n",
    "# Get predictions\n",
    "predicted_ratings=[pred.est for pred in predictions]\n",
    "\n",
    "# Sort predictions\n",
    "\n",
    "list1, list2 = (list(t) for t in zip(*sorted(zip(predicted_ratings, list(iids_to_pred)))))\n",
    "list1.reverse()\n",
    "list2.reverse()\n",
    "predicted_ratings=np.array(list1)\n",
    "iids_to_pred=np.array(list2)\n",
    "\n",
    "# Display predictions\n",
    "\n",
    "num_top=3\n",
    "\n",
    "# if there is a dict of indices to item names that can be used here\n",
    "df_pred=[[uid,iids_to_pred[i],predicted_ratings[i]] for i in range(0,min(num_top,len(iids_to_pred)))]\n",
    "df_pred = pd.DataFrame(df_pred,columns=columns)\n",
    "\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search to tune model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get a full list of tunable SVD parameters [here](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#matrix-factorization-based-algorithms)\n",
    "* Get possible model evaluation metrics [here](https://surprise.readthedocs.io/en/stable/accuracy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_all': 0.1, 'lr_all': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Using grid search CV to get the best model estimates\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import SVD\n",
    "\n",
    "param_grid={'lr_all':[0.001,0.01],'reg_all':[0.1,0.5]}\n",
    "\n",
    "# possible measures - 'rmse','mae','fcp'\n",
    "gs = GridSearchCV(SVD,param_grid,measures=['rmse'],cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_algo = gs.best_estimator['rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.5328  1.9609  0.5228  0.7755  1.7363  1.1057  0.6174  \n",
      "MAE (testset)     0.5328  1.9609  0.5228  0.7755  1.7363  1.1057  0.6174  \n",
      "Fit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n",
      "Test time         0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n"
     ]
    }
   ],
   "source": [
    "# Run 5-fold cross-validation and print results\n",
    "result=cross_validate(final_algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Karthick general procedure of selecting and building a model\n",
    "# How is cross validation applied\n",
    "# More specifically how is training and test data separated\n",
    "# How is final model choosen and scores reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use grid search CV on entire dataset\n",
    "\n",
    "Get best hyper parameters\n",
    "\n",
    "Run cross validate using the best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

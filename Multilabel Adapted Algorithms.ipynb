{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import extmath\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2, GenericUnivariateSelect, mutual_info_classif,f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    dataframe = pd.read_csv(filename, header=None)\n",
    "#     dataframe = dataframe.sample(frac=1)\n",
    "    # takes care of nan values\n",
    "\n",
    "    dataframe=dataframe[:10000].fillna(0)\n",
    "    \n",
    "    dataset = dataframe.values    \n",
    "    header_overall=dataset[0,1:]\n",
    "    header_x=dataset[0,45:]\n",
    "    X = dataset[1:,45:]\n",
    "    Y = dataset[1:,1:45]\n",
    "    #ds.to_csv('compiled_predicted.csv')\n",
    "    return header_overall,header_x,X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahulsuresh/python-environments/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,46,47,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,852,853,854,855,856,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,877,878,879,880,881,882) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "filename=\"/home/rahulsuresh/Downloads/mik_dataset/Model_Sample_Set.csv\"\n",
    "header_overall,header_x,x,y=read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with MIK dataset\n",
    "* Unable to open with libreoffice - so hard but not impossible to visualize\n",
    "* Strings\n",
    "* nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10-NOV-18 12.00.00.000000000 AM', '2018280', '1000171103093', 'N',\n",
       "       '16-FEB-15 12.00.00.000000000 AM',\n",
       "       '01-OCT-18 12.00.00.000000000 AM', '221', '8', '123.42', '82',\n",
       "       '40', '46', '1211071660155', '93.37', '5', '58', '38', '1', '9',\n",
       "       '38', '1', '9', '38', '1', '9', '0', '0', '0', '93.37', '5', '58',\n",
       "       '14.59', '13', '21.19', '65.09', '7.09', '218', 'store_only', '38',\n",
       "       '1', '9', '11.1919', '4', '2', '36', '0', '38', '1', '9',\n",
       "       '11.1919', '4', '2', '36', '0', '38', '1', '9', '11.1919', '4',\n",
       "       '2', '36', '0', '0', '0', '0', '0', '0', '0', '0', '0', 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '1211071660155',\n",
       "       '3', '1', '3', '596', '0', '0', '0', 0, '0', '0', '0', 0, '2', '1',\n",
       "       '2', '40', '0.75', '1', '1', '695', '0', '0', '0', 0, '0', '0',\n",
       "       '0', 0, '0', '0', '0', 0, '0', '0', '0', 0, '0', '0', '0', 0, '0',\n",
       "       '0', '0', 0, '0', '0', '0', 0, '0', '0', '0', 0, '0', '0', '0', 0,\n",
       "       '0', '0', '0', 0, '0', '0', '0', 0, '0', '0', '0', 0, '0', '0',\n",
       "       '0', 0, '0', '0', '0', 0, '0', '0', '0', 0, '4.5', '2', '6', '596',\n",
       "       '0', '0', '0', 0, '7', '2', '11', '596', '0', '0', '0', 0, '0',\n",
       "       '0', '0', 0, '1.59', '1', '1', '531', '4', '1', '2', '40', '3',\n",
       "       '1', '3', '596', '0', '0', '0', 0, '25.15', '3', '15', '40', '0',\n",
       "       '0', '0', 0, '0', '0', '0', 0, '0', '0', '0', 0, '0', '0', '0', 0,\n",
       "       '4.19', '1', '1', '695', '0', '0', '0', 0, '0', '0', '0', 0,\n",
       "       '5.19', '2', '7', '695', '3', '1', '3', '531', '30', '1', '3',\n",
       "       '40', '0', '0', '0', 0, '0', '0', '0', 0, '0', '0', '0', 0, '0',\n",
       "       '0', '0', 0, '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '1',\n",
       "       '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '4', '1', '2', '0', '0', '0', '0', '0', '0', '2', '1',\n",
       "       '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '30', '1', '3', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '2', '1', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '4', '1', '2', '0', '0', '0', '0', '0', '0',\n",
       "       '2', '1', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '30', '1', '3', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '2', '1', '2', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '4', '1', '2', '0', '0', '0', '0',\n",
       "       '0', '0', '2', '1', '2', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '30', '1', '3', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '32.53', '2',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '1', 'C5: Profitables',\n",
       "       '0', '0', '100000', '0', '0', '21-DEC-17 09.21.36.000000000 AM',\n",
       "       '21-DEC-17 09.22.11.000000000 AM', '323.61',\n",
       "       '323.609594907407407407407407407407407407', '143', '1', '1', '143',\n",
       "       '1', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', 0, 0, '0',\n",
       "       '0', '0', '0', '0'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 838)\n",
      "(838,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(header_x.shape)\n",
    "to_delete=[44,45,46,47,48,49,56,81,134,850,851,852,853,854,856,857,858,875]\n",
    "to_delete2 = [ele - 44 for ele in to_delete]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing useless columns - also takes care of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_columns(x,to_delete_list):\n",
    "    for ele in to_delete_list:\n",
    "        x=np.delete(x, np.s_[ele], axis=1)\n",
    "        to_delete_list[:] = [e - 1 for e in to_delete_list]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_columns_vector(a,index):\n",
    "    a=np.delete(a, index)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=delete_columns(x,to_delete2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete2 = [ele - 44 for ele in to_delete]\n",
    "header_x=delete_columns_vector(header_x,to_delete2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtype_linear_shape_y(x,y):\n",
    "    x=x.astype('float')\n",
    "    y=y.astype('float')    \n",
    "    y_new=[]\n",
    "    for row in y:\n",
    "        y_new.append(np.argmax(row))\n",
    "    return x,np.array(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_=convert_dtype_linear_shape_y(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 -> generate a sample multilabel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "# x,y=make_multilabel_classification(n_samples=8000, n_features=80, \n",
    "#                                              n_classes=10, n_labels=4, length=100, \n",
    "#                                              allow_unlabeled=True, sparse=False, return_indicator=\"dense\", \n",
    "#                                              return_distributions=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y,test_size = .2,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 820)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def label_counter(y):\n",
    "    return np.unique(y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def to_multilabel_csr(y):   \n",
    "    multilabel_binarizer = MultiLabelBinarizer()\n",
    "    multilabel_binarizer.fit(y)\n",
    "    y = multilabel_binarizer.transform(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(x,y,acc_list,labels):\n",
    "    x_final=[]\n",
    "    y_final=[]\n",
    "    for i in acc_list:\n",
    "        search_label=labels[i]\n",
    "        for j in range(len(y)):\n",
    "            if(y[j]==search_label):\n",
    "                x_final.append(x[j])\n",
    "                y_final.append(y[j])\n",
    "    return x_final,y_final    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.sparse import csr_matrix\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Problem : Encoding sample has a single unique appearance\n",
    "\n",
    "def smote_sampler(x_train,yt,lp):\n",
    "    labels,vals=label_counter(yt)\n",
    "\n",
    "    ratio_list=[]\n",
    "    delete_list=[]\n",
    "    \n",
    "    for i in range(len(np.unique(yt))):\n",
    "        if (vals[i]>6):\n",
    "            ratio_list.append(i)\n",
    "        else:\n",
    "            delete_list.append(i)\n",
    "\n",
    "    x_train_sm,yt_sm=get_data(x_train,yt,ratio_list,labels)\n",
    "\n",
    "    x_train_rem,yt_rem=get_data(x_train,yt,delete_list,labels)\n",
    "            \n",
    "    sm = SMOTE(random_state=42)\n",
    "\n",
    "    x_res, y_res = sm.fit_sample(x_train_sm, yt_sm)\n",
    "\n",
    "    y_res = lp.inverse_transform(y_res)\n",
    "        \n",
    "    yt_rem = lp.inverse_transform(yt_rem)\n",
    "    \n",
    "    arr=np.concatenate((y_res.rows, yt_rem.rows), axis=0)\n",
    "    \n",
    "    y_res=to_multilabel_csr(arr)\n",
    "    \n",
    "    return np.concatenate((x_res, x_train_rem), axis=0),y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from scipy.sparse import csr_matrix\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Problem : Encoding sample has a single unique appearance\n",
    "\n",
    "def smoteenn_sampler(x_train,yt,lp):\n",
    "    labels,vals=label_counter(yt)\n",
    "\n",
    "    ratio_list=[]\n",
    "    delete_list=[]\n",
    "    \n",
    "    for i in range(len(np.unique(yt))):\n",
    "        if (vals[i]>6):\n",
    "            ratio_list.append(i)\n",
    "        else:\n",
    "            delete_list.append(i)\n",
    "\n",
    "    x_train_sm,yt_sm=get_data(x_train,yt,ratio_list,labels)\n",
    "\n",
    "    x_train_rem,yt_rem=get_data(x_train,yt,delete_list,labels)\n",
    "            \n",
    "    sm = SMOTEENN(random_state=42)\n",
    "\n",
    "    x_res, y_res = sm.fit_sample(x_train_sm, yt_sm)\n",
    "\n",
    "    y_res = lp.inverse_transform(y_res)\n",
    "        \n",
    "    yt_rem = lp.inverse_transform(yt_rem)\n",
    "    \n",
    "    arr=np.concatenate((y_res.rows, yt_rem.rows), axis=0)\n",
    "    \n",
    "    y_res=to_multilabel_csr(arr)\n",
    "    \n",
    "    return np.concatenate((x_res, x_train_rem), axis=0),y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "lp = LabelPowerset()\n",
    "\n",
    "# Applies the above stated multi-label (ML) to multi-class (MC) transformation.\n",
    "yt = lp.transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_res,y_res=smote_sampler(x_train,yt,lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_res2,y_res2=smoteenn_sampler(x_train,yt,lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Algorithm        | Data Oversampling technique | Accuracy(do x 100 for %) | Hamming Loss |\n",
    "| :-------------: |:--------:|:-----:|:-----:|\n",
    "| MultiLearn KNN (MLKNN)     | None | 0.004 | 0.05646590909090909 |\n",
    "| Binary Relevance a KNN (BRaKNN)     | None | 0.001 |  0.055625 |\n",
    "| Binary Relevance b KNN (BRbKNN)     | None | 0.0 |  0.10853409090909091 |\n",
    "| Multi-label ARAM (MLARAM)     | None | 0.0 |  0.10853409090909091 |\n",
    "| Decision Tree     | None | 0.0 |  0.10853409090909091 |\n",
    "| Extra Tree     | None | 0.0 |  0.10853409090909091 |\n",
    "| Extra Tree Ensemble     | None | 0.0 |  0.10853409090909091 |\n",
    "| Random Forest     | None | 0.0 |  0.10853409090909091 |\n",
    "| | | | |\n",
    "| MultiLearn KNN (MLKNN)     | SMOTE | 0.0045 | 0.0655340909090909 |\n",
    "| Binary Relevance a KNN (BRaKNN)     | SMOTE | 0.004 |  0.06780681818181818 |\n",
    "| Binary Relevance b KNN (BRbKNN)     | SMOTE | 0.0005 |  0.09689772727272727 |\n",
    "| Multi-label ARAM (MLARAM)     | SMOTE | 0.0005 |  0.09689772727272727 |\n",
    "| Decision Tree     | SMOTE | 0.0005 |  0.10853409090909091 |\n",
    "| Extra Tree     | SMOTE | 0.0005 |  0.10853409090909091 |\n",
    "| Extra Tree Ensemble     | SMOTE | 0.0005 |  0.10853409090909091 |\n",
    "| Random Forest     | SMOTE | 0.0005 |  0.10853409090909091 |\n",
    "| | | | |\n",
    "| MultiLearn KNN (MLKNN)     | SMOTEENN | 0.0045 | 0.06894318181818182 |\n",
    "| Binary Relevance a KNN (BRaKNN)     | SMOTEENN | 0.004 |  0.07055681818181818 |\n",
    "| Binary Relevance b KNN (BRbKNN)     | SMOTEENN | 0.0005 |   0.09932954545454545 |\n",
    "| Multi-label ARAM (MLARAM)     | SMOTEENN | 0.0005 |   0.09932954545454545 |\n",
    "| Decision Tree     | SMOTEENN | 0.0005 |   0.09932954545454545 |\n",
    "| Extra Tree     | SMOTEENN | 0.0005 |   0.09932954545454545 |\n",
    "| Extra Tree Ensemble     | SMOTEENN | 0.0005 |   0.09932954545454545 |\n",
    "| Random Forest     | SMOTEENN | 0.0005 |   0.09932954545454545 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLearn KNN (MLKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0045 Hamming Loss: 0.06894318181818182\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.metrics import accuracy_score,hamming_loss\n",
    "\n",
    "classifier = MLkNN(k=10)\n",
    "\n",
    "# train\n",
    "classifier.fit(x_res2, y_res2)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_val)\n",
    "\n",
    "acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Relevance a KNN (BRaKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.004 Hamming Loss: 0.07055681818181818\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import BRkNNaClassifier\n",
    "from sklearn.metrics import accuracy_score,hamming_loss\n",
    "\n",
    "classifier = BRkNNaClassifier(k=10)\n",
    "\n",
    "# train\n",
    "classifier.fit(x_res2, y_res2)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_val)\n",
    "\n",
    "acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Relevance b KNN (BRbKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0005 Hamming Loss: 0.09932954545454545\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import BRkNNbClassifier\n",
    "from sklearn.metrics import accuracy_score,hamming_loss\n",
    "\n",
    "classifier = BRkNNbClassifier(k=10)\n",
    "\n",
    "# train\n",
    "classifier.fit(x_res2, y_res2)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_val)\n",
    "\n",
    "acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label ARAM (MLARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0005 Hamming Loss: 0.09932954545454545\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLARAM\n",
    "from sklearn.metrics import accuracy_score,hamming_loss\n",
    "\n",
    "classifier = MLARAM(threshold=0.05, vigilance=0.95)\n",
    "classifier.fit(x_res2, y_res2)\n",
    "prediction = classifier.predict(x_val)\n",
    "\n",
    "acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twin Multi Label Support Vector Machines (MLTSVM)   -> Not working; No support from sk multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skmultilearn.adapt import MLTSVM\n",
    "# from sklearn.metrics import accuracy_score,hamming_loss\n",
    "\n",
    "# classifier = MLTSVM()\n",
    "# classifier.fit(x_train, y_train)\n",
    "# prediction = classifier.predict(x_val)\n",
    "\n",
    "# acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "# ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "# print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0005 Hamming Loss: 0.09932954545454545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(x_res2, y_res2)\n",
    "prediction = classifier.predict(x_val)\n",
    "\n",
    "acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0005 Hamming Loss: 0.09932954545454545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "classifier = ExtraTreeClassifier()\n",
    "classifier.fit(x_res2, y_res2)\n",
    "prediction = classifier.predict(x_val)\n",
    "\n",
    "acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahulsuresh/python-environments/env/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0005 Hamming Loss: 0.09932954545454545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "classifier = ExtraTreesClassifier()\n",
    "classifier.fit(x_res2, y_res2)\n",
    "prediction = classifier.predict(x_val)\n",
    "\n",
    "acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP -> Not working due to label issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# classifier = MLPClassifier()\n",
    "# classifier.fit(x_train, y_train)\n",
    "# prediction = classifier.predict(x_val)\n",
    "\n",
    "# acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "# ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "# print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radius Neighbors Classifier -> Not Working because of absence of neigbours for many samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "# classifier = RadiusNeighborsClassifier()\n",
    "# classifier.fit(x_train, y_train)\n",
    "# prediction = classifier.predict(x_val)\n",
    "\n",
    "# acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "# ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "# print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahulsuresh/python-environments/env/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0005 Hamming Loss: 0.09932954545454545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(x_res2, y_res2)\n",
    "prediction = classifier.predict(x_val)\n",
    "\n",
    "acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier -> Not working due to label issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import RidgeClassifierCV\n",
    "\n",
    "# classifier = RidgeClassifierCV()\n",
    "# classifier.fit(x_train, y_train)\n",
    "# prediction = classifier.predict(x_val)\n",
    "\n",
    "# acc=accuracy_score(y_val,predictions)\n",
    "\n",
    "# ham=hamming_loss(y_val, predictions)\n",
    "\n",
    "# print(\"Accuracy:\",acc,\"Hamming Loss:\",ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
